{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2113f2a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating run_2\n",
      "Using full dataset\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "from config import *\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
    "\n",
    "print(f\"Evaluating {RUN}\")\n",
    "print(f\"Sampling subset (n={SUBSET_SIZE})\" if USE_SUBSET else \"Using full dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cdce09c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Loading FAISS index and metadata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded with 20260 vectors\n",
      "\n",
      "> Loading QA model\n",
      "QA model loaded on device: cuda\n",
      "\n",
      "> Loading datasets\n",
      "Dataset sizes:\n",
      "  Train: 10207 examples\n",
      "  Validation: 2187 examples\n",
      "  Test: 2188 examples\n"
     ]
    }
   ],
   "source": [
    "# Load FAISS index and metadata for retrieval\n",
    "print(\"> Loading FAISS index and metadata\")\n",
    "index = faiss.read_index(FAISS_INDEX_PATH)\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "with open(METADATA_PATH, \"rb\") as f:\n",
    "    metadata = pickle.load(f)\n",
    "print(f\"FAISS index loaded with {index.ntotal} vectors\")\n",
    "print()\n",
    "\n",
    "# Load QA model and tokenizer\n",
    "print(\"> Loading QA model\")\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device\n",
    ")\n",
    "print(f\"QA model loaded on device: {'cuda' if device == 0 else 'cpu'}\\n\")\n",
    "\n",
    "# Load datasets for evaluation\n",
    "print(\"> Loading datasets\")\n",
    "train_dataset = load_dataset(\"json\", data_files=TRAIN_DATA_PATH)[\"train\"]\n",
    "val_dataset = load_dataset(\"json\", data_files=VAL_DATA_PATH)[\"train\"]  \n",
    "test_dataset = load_dataset(\"json\", data_files=TEST_DATA_PATH)[\"train\"]\n",
    "\n",
    "print(f\"Dataset sizes:\")\n",
    "print(f\"  Train: {len(train_dataset)} examples\")\n",
    "print(f\"  Validation: {len(val_dataset)} examples\") \n",
    "print(f\"  Test: {len(test_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5c1e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation utility functions\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lowercase, remove punctuation, articles, and extra whitespace.\"\"\"\n",
    "    s = s.lower()\n",
    "    s = ''.join(ch for ch in s if ch not in string.punctuation)\n",
    "    s = re.sub(r'\\b(a|an|the)\\b', ' ', s)\n",
    "    s = ' '.join(s.split())\n",
    "    return s\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "def evaluate_dataset(dataset, use_retrieval=False, max_examples=None, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with questions, contexts, and answers\n",
    "        use_retrieval: If True, use FAISS retrieval to get context. If False, use original context\n",
    "        max_examples: Limit evaluation to this many examples (for faster testing)\n",
    "        dataset_name: Name for logging purposes\n",
    "    \"\"\"\n",
    "    if max_examples:\n",
    "        dataset = dataset.select(range(min(max_examples, len(dataset))))\n",
    "    \n",
    "    print(f\"\\n=== Evaluating {dataset_name} ({len(dataset)} examples) ===\")\n",
    "    print(f\"Using {'retrieval + QA' if use_retrieval else 'direct QA'} pipeline\")\n",
    "    \n",
    "    exact_matches = []\n",
    "    f1_scores = []\n",
    "    predictions = []\n",
    "    \n",
    "    for i, example in enumerate(dataset):\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Processing example {i+1}/{len(dataset)}\")\n",
    "        \n",
    "        question = example['question']\n",
    "        gold_answers = example['answers']['text']\n",
    "        \n",
    "        if use_retrieval:\n",
    "            # Use retrieval like in the API\n",
    "            question_embedding = embedding_model.encode([question], convert_to_numpy=True)\n",
    "            distances, indices = index.search(question_embedding, k=3)\n",
    "            context = \" \".join([metadata[j]['answer_chunk'] for j in indices[0]])\n",
    "        else:\n",
    "            # Use original context from dataset\n",
    "            context = example['context']\n",
    "        \n",
    "        try:\n",
    "            # Get prediction from QA pipeline\n",
    "            result = qa_pipeline(question=question, context=context)\n",
    "            predicted_answer = result['answer']\n",
    "            confidence = result['score']\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {e}\")\n",
    "            predicted_answer = \"\"\n",
    "            confidence = 0.0\n",
    "        \n",
    "        # Compute metrics against all gold answers (take max)\n",
    "        max_exact = 0\n",
    "        max_f1 = 0\n",
    "        for gold_answer in gold_answers:\n",
    "            exact = compute_exact(gold_answer, predicted_answer)\n",
    "            f1 = compute_f1(gold_answer, predicted_answer)\n",
    "            max_exact = max(max_exact, exact)\n",
    "            max_f1 = max(max_f1, f1)\n",
    "        \n",
    "        exact_matches.append(max_exact)\n",
    "        f1_scores.append(max_f1)\n",
    "        predictions.append({\n",
    "            'question': question,\n",
    "            'predicted_answer': predicted_answer,\n",
    "            'gold_answers': gold_answers,\n",
    "            'confidence': confidence,\n",
    "            'exact_match': max_exact,\n",
    "            'f1_score': max_f1,\n",
    "            'context_used': context[:200] + \"...\" if len(context) > 200 else context\n",
    "        })\n",
    "    \n",
    "    # Compute final metrics\n",
    "    avg_exact = np.mean(exact_matches) * 100\n",
    "    avg_f1 = np.mean(f1_scores) * 100\n",
    "    \n",
    "    print(f\"\\n> Results for {dataset_name}:\")\n",
    "    print(f\"  Exact Match: {avg_exact:.2f}%\")\n",
    "    print(f\"  F1 Score: {avg_f1:.2f}%\")\n",
    "    print(f\"  Total examples: {len(exact_matches)}\\n\")\n",
    "    \n",
    "    return {\n",
    "        'exact_match': avg_exact,\n",
    "        'f1_score': avg_f1,\n",
    "        'predictions': predictions,\n",
    "        'num_examples': len(exact_matches)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adcf6012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Starting comprehensive model evaluation\n",
      "\n",
      "============================================================\n",
      "EVALUATING ON TEST SET\n",
      "============================================================\n",
      "\n",
      "=== Evaluating Test Set (Direct QA) (100 examples) ===\n",
      "Using direct QA pipeline\n",
      "Processing example 1/100\n",
      "Processing example 51/100\n",
      "\n",
      "> Results for Test Set (Direct QA):\n",
      "  Exact Match: 3.00%\n",
      "  F1 Score: 8.90%\n",
      "  Total examples: 100\n",
      "\n",
      "\n",
      "=== Evaluating Test Set (Retrieval + QA) (100 examples) ===\n",
      "Using retrieval + QA pipeline\n",
      "Processing example 1/100\n",
      "Processing example 51/100\n",
      "\n",
      "> Results for Test Set (Retrieval + QA):\n",
      "  Exact Match: 0.00%\n",
      "  F1 Score: 5.78%\n",
      "  Total examples: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run comprehensive evaluation on all datasets\n",
    "print(\"> Starting comprehensive model evaluation\")\n",
    "\n",
    "# For faster testing, limit to smaller subsets initially\n",
    "# Set max_examples=None to evaluate on full datasets\n",
    "MAX_EXAMPLES = 100  # Adjust as needed\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Evaluate on test set (most important)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Direct QA evaluation (using original contexts)\n",
    "results['test_direct'] = evaluate_dataset(\n",
    "    test_dataset, \n",
    "    use_retrieval=False, \n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    dataset_name=\"Test Set (Direct QA)\"\n",
    ")\n",
    "\n",
    "# Retrieval + QA evaluation (like the API)\n",
    "results['test_retrieval'] = evaluate_dataset(\n",
    "    test_dataset, \n",
    "    use_retrieval=True, \n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    dataset_name=\"Test Set (Retrieval + QA)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c147b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "   EVALUATING ON VALIDATION SET\n",
      "============================================================\n",
      "\n",
      "=== Evaluating Validation Set (Direct QA) (100 examples) ===\n",
      "Using direct QA pipeline\n",
      "Processing example 1/100\n",
      "Processing example 51/100\n",
      "\n",
      "> Results for Validation Set (Direct QA):\n",
      "  Exact Match: 0.00%\n",
      "  F1 Score: 5.57%\n",
      "  Total examples: 100\n",
      "\n",
      "\n",
      "=== Evaluating Validation Set (Retrieval + QA) (100 examples) ===\n",
      "Using retrieval + QA pipeline\n",
      "Processing example 1/100\n",
      "Processing example 51/100\n",
      "\n",
      "> Results for Validation Set (Retrieval + QA):\n",
      "  Exact Match: 0.00%\n",
      "  F1 Score: 4.58%\n",
      "  Total examples: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on validation set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"   EVALUATING ON VALIDATION SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results['val_direct'] = evaluate_dataset(\n",
    "    val_dataset, \n",
    "    use_retrieval=False, \n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    dataset_name=\"Validation Set (Direct QA)\"\n",
    ")\n",
    "\n",
    "results['val_retrieval'] = evaluate_dataset(\n",
    "    val_dataset, \n",
    "    use_retrieval=True, \n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    dataset_name=\"Validation Set (Retrieval + QA)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f8bbc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "   EVALUATING ON TRAINING SET\n",
      "============================================================\n",
      "\n",
      "=== Evaluating Training Set (Direct QA) (100 examples) ===\n",
      "Using direct QA pipeline\n",
      "Processing example 1/100\n",
      "Processing example 51/100\n",
      "\n",
      "> Results for Training Set (Direct QA):\n",
      "  Exact Match: 1.00%\n",
      "  F1 Score: 9.72%\n",
      "  Total examples: 100\n",
      "\n",
      "\n",
      "=== Evaluating Training Set (Retrieval + QA) (100 examples) ===\n",
      "Using retrieval + QA pipeline\n",
      "Processing example 1/100\n",
      "Processing example 51/100\n",
      "\n",
      "> Results for Training Set (Retrieval + QA):\n",
      "  Exact Match: 0.00%\n",
      "  F1 Score: 5.31%\n",
      "  Total examples: 100\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on training set (to check for overfitting)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"   EVALUATING ON TRAINING SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results['train_direct'] = evaluate_dataset(\n",
    "    train_dataset, \n",
    "    use_retrieval=False, \n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    dataset_name=\"Training Set (Direct QA)\"\n",
    ")\n",
    "\n",
    "results['train_retrieval'] = evaluate_dataset(\n",
    "    train_dataset, \n",
    "    use_retrieval=True, \n",
    "    max_examples=MAX_EXAMPLES,\n",
    "    dataset_name=\"Training Set (Retrieval + QA)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c552b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   COMPREHENSIVE EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Dataset         Method          Exact Match  F1 Score   Examples  \n",
      "----------------------------------------------------------------------\n",
      "Test            Direct QA       3.00        % 8.90      % 100       \n",
      "Test            Retrieval+QA    0.00        % 5.78      % 100       \n",
      "Val             Direct QA       0.00        % 5.57      % 100       \n",
      "Val             Retrieval+QA    0.00        % 4.58      % 100       \n",
      "Train           Direct QA       1.00        % 9.72      % 100       \n",
      "Train           Retrieval+QA    0.00        % 5.31      % 100       \n",
      "\n",
      "> Overfitting Analysis (Direct QA):\n",
      "   - Training F1: 9.72%\n",
      "   - Test F1: 8.90%\n",
      "   - Gap: 0.83%\n",
      "   - Good generalization!\n"
     ]
    }
   ],
   "source": [
    "# Summary of all results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   COMPREHENSIVE EVALUATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a results table\n",
    "print(f\"{'Dataset':<15} {'Method':<15} {'Exact Match':<12} {'F1 Score':<10} {'Examples':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for key, result in results.items():\n",
    "    dataset_name, method = key.split('_')\n",
    "    dataset_name = dataset_name.capitalize()\n",
    "    method = \"Direct QA\" if method == \"direct\" else \"Retrieval+QA\"\n",
    "    \n",
    "    print(f\"{dataset_name:<15} {method:<15} {result['exact_match']:<12.2f}% {result['f1_score']:<10.2f}% {result['num_examples']:<10}\")\n",
    "# Check for overfitting\n",
    "if 'train_direct' in results and 'test_direct' in results:\n",
    "    train_f1 = results['train_direct']['f1_score']\n",
    "    test_f1 = results['test_direct']['f1_score']\n",
    "    overfitting_gap = train_f1 - test_f1\n",
    "    print(f\"\\n> Overfitting Analysis (Direct QA):\")\n",
    "    print(f\"   - Training F1: {train_f1:.2f}%\")\n",
    "    print(f\"   - Test F1: {test_f1:.2f}%\")\n",
    "    print(f\"   - Gap: {overfitting_gap:.2f}%\")\n",
    "    \n",
    "    if overfitting_gap > 10:\n",
    "        print(\"   - Significant overfitting detected!\")\n",
    "    elif overfitting_gap > 5:\n",
    "        print(\"   - Moderate overfitting detected\")\n",
    "    else:\n",
    "        print(\"   - Good generalization!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "011b5f5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "   EXAMPLE PREDICTIONS\n",
      "================================================================================\n",
      "\n",
      "--- Test Direct Examples ---\n",
      "\n",
      "Example 1 - O - CORRECT\n",
      "Question: How many people are affected by cystinuria ?\n",
      "Gold Answer(s): ['Cystinuria affects approximately 1 in 10,000 people.']\n",
      "Predicted: 'Cystinuria affects approximately 1 in 10,000 people.'\n",
      "Confidence: 0.9999\n",
      "F1 Score: 1.00\n",
      "Context: Cystinuria affects approximately 1 in 10,000 people.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Example 2 - X - INCORRECT\n",
      "Question: Is Galactosialidosis inherited ?\n",
      "Gold Answer(s): ['How is galactosialidosis inherited? Galactosialidosis is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. The parents of an individual with an autosomal recessive condition each carry one copy of the mutated gene, but they typically do not show signs and symptoms of the condition.']\n",
      "Predicted: 'How is galactosialidosis inherited?'\n",
      "Confidence: 0.0011\n",
      "F1 Score: 0.16\n",
      "Context: How is galactosialidosis inherited? Galactosialidosis is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. The parents of an individual with...\n",
      "------------------------------------------------------------\n",
      "\n",
      "--- Test Retrieval Examples ---\n",
      "\n",
      "Example 1 - X - INCORRECT\n",
      "Question: Is Galactosialidosis inherited ?\n",
      "Gold Answer(s): ['How is galactosialidosis inherited? Galactosialidosis is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. The parents of an individual with an autosomal recessive condition each carry one copy of the mutated gene, but they typically do not show signs and symptoms of the condition.']\n",
      "Predicted: 'how is galactosialidosis inherited?'\n",
      "Confidence: 0.0077\n",
      "F1 Score: 0.16\n",
      "Context: how is galactosialidosis inherited? galactosialidosis is inherited in an autosomal recessive pattern, which means both copies of the gene in each cell have mutations. the parents of an individual with...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show example predictions\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   EXAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def show_examples(results_key, num_examples=5):\n",
    "    if results_key not in results:\n",
    "        print(f\"No results found for {results_key}\")\n",
    "        return\n",
    "    \n",
    "    predictions = results[results_key]['predictions']\n",
    "    dataset_name = results_key.replace('_', ' ').title()\n",
    "    \n",
    "    print(f\"\\n--- {dataset_name} Examples ---\")\n",
    "    \n",
    "    # Show a mix of correct and incorrect predictions\n",
    "    correct_preds = [p for p in predictions if p['exact_match'] == 1]\n",
    "    incorrect_preds = [p for p in predictions if p['exact_match'] == 0]\n",
    "    \n",
    "    examples_to_show = []\n",
    "    examples_to_show.extend(correct_preds[:num_examples//2])\n",
    "    examples_to_show.extend(incorrect_preds[:num_examples//2])\n",
    "    \n",
    "    for i, pred in enumerate(examples_to_show[:num_examples]):\n",
    "        status = \"O - CORRECT\" if pred['exact_match'] == 1 else \"X - INCORRECT\"\n",
    "        print(f\"\\nExample {i+1} - {status}\")\n",
    "        print(f\"Question: {pred['question']}\")\n",
    "        print(f\"Gold Answer(s): {pred['gold_answers']}\")\n",
    "        print(f\"Predicted: '{pred['predicted_answer']}'\")\n",
    "        print(f\"Confidence: {pred['confidence']:.4f}\")\n",
    "        print(f\"F1 Score: {pred['f1_score']:.2f}\")\n",
    "        print(f\"Context: {pred['context_used']}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "# Show examples from test set\n",
    "show_examples('test_direct', num_examples=3)\n",
    "show_examples('test_retrieval', num_examples=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5284685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Evaluation results saved to ../models/evaluation_results.json\n",
      "\n",
      "================================================================================\n",
      "   RECOMMENDATIONS\n",
      "================================================================================\n",
      "> Test Set Performance:\n",
      "   Direct QA F1: 8.90%\n",
      "   Retrieval+QA F1: 5.78%\n",
      "\n",
      "> Retrieval Performance Gap: 3.12%\n",
      "   - Retrieval system performing reasonably well\n",
      "\n",
      "> Overall Assessment:\n",
      "   - Poor performance - requires significant work\n"
     ]
    }
   ],
   "source": [
    "# Save evaluation results\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Prepare results for saving (remove predictions to reduce file size)\n",
    "results_summary = {}\n",
    "for key, result in results.items():\n",
    "    results_summary[key] = {\n",
    "        'exact_match': result['exact_match'],\n",
    "        'f1_score': result['f1_score'],\n",
    "        'num_examples': result['num_examples']\n",
    "    }\n",
    "\n",
    "# Add metadata\n",
    "evaluation_metadata = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'model_path': MODEL_PATH,\n",
    "    'embedding_model': EMBEDDING_MODEL_NAME,\n",
    "    'max_examples_per_dataset': MAX_EXAMPLES,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'results': results_summary\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "results_path = '../models/evaluation_results.json'\n",
    "with open(results_path, 'w') as f:\n",
    "    json.dump(evaluation_metadata, f, indent=2)\n",
    "\n",
    "print(f\"> Evaluation results saved to {results_path}\")\n",
    "\n",
    "# Final recommendations\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"   RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_retrieval_f1 = results.get('test_retrieval', {}).get('f1_score', 0)\n",
    "test_direct_f1 = results.get('test_direct', {}).get('f1_score', 0)\n",
    "\n",
    "print(f\"> Test Set Performance:\")\n",
    "print(f\"   Direct QA F1: {test_direct_f1:.2f}%\")\n",
    "print(f\"   Retrieval+QA F1: {test_retrieval_f1:.2f}%\")\n",
    "\n",
    "if test_direct_f1 > test_retrieval_f1:\n",
    "    gap = test_direct_f1 - test_retrieval_f1\n",
    "    print(f\"\\n> Retrieval Performance Gap: {gap:.2f}%\")\n",
    "    if gap > 10:\n",
    "        print(\"   - Large gap suggests retrieval system needs improvement\")\n",
    "        print(\"   - Consider: Better embedding model, larger k, improved chunking\")\n",
    "    else:\n",
    "        print(\"   - Retrieval system performing reasonably well\")\n",
    "\n",
    "print(f\"\\n> Overall Assessment:\")\n",
    "if test_retrieval_f1 > 70:\n",
    "    print(\"   - Excellent performance - ready for production\")\n",
    "elif test_retrieval_f1 > 50:\n",
    "    print(\"   - Good performance - consider fine-tuning\")\n",
    "elif test_retrieval_f1 > 30:\n",
    "    print(\"   - Moderate performance - needs improvement\")\n",
    "else:\n",
    "    print(\"   - Poor performance - requires significant work\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
