{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e74a89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_index</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>Glaucoma is a group of diseases that can damag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>The optic nerve is a bundle of more than 1 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>Open-angle glaucoma is the most common form of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Who is at risk for Glaucoma? ?</td>\n",
       "      <td>Anyone can develop glaucoma. Some people are a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How to prevent Glaucoma ?</td>\n",
       "      <td>At this time, we do not know how to prevent gl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original_index                        question  \\\n",
       "0               0        What is (are) Glaucoma ?   \n",
       "1               1        What is (are) Glaucoma ?   \n",
       "2               2        What is (are) Glaucoma ?   \n",
       "3               3  Who is at risk for Glaucoma? ?   \n",
       "4               4       How to prevent Glaucoma ?   \n",
       "\n",
       "                                              answer  \n",
       "0  Glaucoma is a group of diseases that can damag...  \n",
       "1  The optic nerve is a bundle of more than 1 mil...  \n",
       "2  Open-angle glaucoma is the most common form of...  \n",
       "3  Anyone can develop glaucoma. Some people are a...  \n",
       "4  At this time, we do not know how to prevent gl...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset and rename the original index column to \"original index\"\n",
    "df = pd.read_csv('../data/intern_screening_dataset.csv').reset_index().rename(columns={'index': 'original_index'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0d4a7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['original_index', 'question', 'answer']\n",
      "Data types: ['int64', 'object', 'object']\n",
      "Count: 16406\n",
      "Missing values in question: 0\n",
      "Missing values in answer: 5\n",
      "Unique values in question: 14981\n",
      "Unique values in answer: 15811\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and data types\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"Data types:\", [str(dt) for dt in df.dtypes])\n",
    "print(\"Count:\", df['question'].count())\n",
    "print(\"Missing values in question:\", df['question'].isnull().sum())\n",
    "print(\"Missing values in answer:\", df['answer'].isnull().sum())\n",
    "print(\"Unique values in question:\", df['question'].nunique())\n",
    "print(\"Unique values in answer:\", df['answer'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a26c48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer length statistics:\n",
      "  Max length: 29046\n",
      "  Min length: 0\n",
      "  Mean length: 1302.61\n",
      "  Median length: 889.0\n",
      "  Standard deviation: 1656.13\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics about the length of the answers\n",
    "answer_lengths = df['answer'].apply(lambda x: \"\" if pd.isna(x) else x).apply(len)\n",
    "print(\"Answer length statistics:\")\n",
    "print(f\"  Max length: {answer_lengths.max()}\")\n",
    "print(f\"  Min length: {answer_lengths.min()}\")\n",
    "print(f\"  Mean length: {answer_lengths.mean():.2f}\")\n",
    "print(f\"  Median length: {answer_lengths.median()}\")\n",
    "print(f\"  Standard deviation: {answer_lengths.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bbbf896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>original_index</th>\n",
       "      <th>question_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Do you have information about A1C</td>\n",
       "      <td>Summary : A1C is a blood test for type 2 diabe...</td>\n",
       "      <td>[1909]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Do you have information about Acupuncture</td>\n",
       "      <td>Summary : Acupuncture has been practiced in Ch...</td>\n",
       "      <td>[1744]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Do you have information about Adoption</td>\n",
       "      <td>Summary : Adoption brings a child born to othe...</td>\n",
       "      <td>[2307]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Do you have information about Advance Directives</td>\n",
       "      <td>Summary : What kind of medical care would you ...</td>\n",
       "      <td>[1937]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do you have information about African American...</td>\n",
       "      <td>Summary : Every racial or ethnic group has spe...</td>\n",
       "      <td>[2434]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                  Do you have information about A1C   \n",
       "1          Do you have information about Acupuncture   \n",
       "2             Do you have information about Adoption   \n",
       "3   Do you have information about Advance Directives   \n",
       "4  Do you have information about African American...   \n",
       "\n",
       "                                              answer original_index  \\\n",
       "0  Summary : A1C is a blood test for type 2 diabe...         [1909]   \n",
       "1  Summary : Acupuncture has been practiced in Ch...         [1744]   \n",
       "2  Summary : Adoption brings a child born to othe...         [2307]   \n",
       "3  Summary : What kind of medical care would you ...         [1937]   \n",
       "4  Summary : Every racial or ethnic group has spe...         [2434]   \n",
       "\n",
       "   question_index  \n",
       "0               0  \n",
       "1               1  \n",
       "2               2  \n",
       "3               3  \n",
       "4               4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows where 'answer' is missing\n",
    "df_clean = df.dropna(subset=['answer'])\n",
    "\n",
    "# For each question, concatenate all answers and collect the list of original indices\n",
    "df_concat = df_clean.groupby('question').agg({\n",
    "    'answer': lambda x: '\\n---\\n'.join(x),\n",
    "    'original_index': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Create a new column 'question_index' with the index of each unique question\n",
    "df_concat['question_index'] = df_concat.index\n",
    "\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ca77d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: ['question', 'answer', 'original_index', 'question_index']\n",
      "Data types: ['object', 'object', 'object', 'int64']\n",
      "Count: 14976\n",
      "Missing values in question: 0\n",
      "Missing values in answer: 0\n",
      "Unique values in question: 14976\n",
      "Unique values in answer: 14470\n"
     ]
    }
   ],
   "source": [
    "# Check the columns and data types\n",
    "print(\"Columns:\", list(df_concat.columns))\n",
    "print(\"Data types:\", [str(dt) for dt in df_concat.dtypes])\n",
    "print(\"Count:\", df_concat['question'].count())\n",
    "print(\"Missing values in question:\", df_concat['question'].isnull().sum())\n",
    "print(\"Missing values in answer:\", df_concat['answer'].isnull().sum())\n",
    "print(\"Unique values in question:\", df_concat['question'].nunique())\n",
    "print(\"Unique values in answer:\", df_concat['answer'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb0eb330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save a mapping from question_index to question and original_index\n",
    "question_map = df_concat[['question_index', 'question', 'original_index']].set_index('question_index').to_dict(orient='index')\n",
    "with open('../data/question_index_map.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(question_map, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b21a3808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>original_index</th>\n",
       "      <th>question_index</th>\n",
       "      <th>answer_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A kidney stone is a solid piece of material ...</td>\n",
       "      <td>What to do for Kidney Stones in Adults ?</td>\n",
       "      <td>[15613]</td>\n",
       "      <td>[14255]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- A person may prevent or delay some health pr...</td>\n",
       "      <td>What to do for Nutrition for Advanced Chronic ...</td>\n",
       "      <td>[15567]</td>\n",
       "      <td>[14268]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>- Acromegaly is a hormonal disorder that resul...</td>\n",
       "      <td>What to do for Acromegaly ?</td>\n",
       "      <td>[15719]</td>\n",
       "      <td>[14196]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>- Bladder problems have many possible causes. ...</td>\n",
       "      <td>What to do for What I need to know about Inter...</td>\n",
       "      <td>[16186]</td>\n",
       "      <td>[14315]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>- Cirrhosis is scarring of the liver. Scar tis...</td>\n",
       "      <td>What to do for What I need to know about Cirrh...</td>\n",
       "      <td>[15262]</td>\n",
       "      <td>[14304]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  \\\n",
       "0  - A kidney stone is a solid piece of material ...   \n",
       "1  - A person may prevent or delay some health pr...   \n",
       "2  - Acromegaly is a hormonal disorder that resul...   \n",
       "3  - Bladder problems have many possible causes. ...   \n",
       "4  - Cirrhosis is scarring of the liver. Scar tis...   \n",
       "\n",
       "                                            question original_index  \\\n",
       "0           What to do for Kidney Stones in Adults ?        [15613]   \n",
       "1  What to do for Nutrition for Advanced Chronic ...        [15567]   \n",
       "2                        What to do for Acromegaly ?        [15719]   \n",
       "3  What to do for What I need to know about Inter...        [16186]   \n",
       "4  What to do for What I need to know about Cirrh...        [15262]   \n",
       "\n",
       "  question_index  answer_index  \n",
       "0        [14255]             0  \n",
       "1        [14268]             1  \n",
       "2        [14196]             2  \n",
       "3        [14315]             3  \n",
       "4        [14304]             4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Group by 'answer' to aggregate duplicated answers\n",
    "df_answer_grouped = df_concat.groupby('answer').agg({\n",
    "    # Concatenate all unique questions into a single string, separated by '\\n---\\n'\n",
    "    'question': lambda x: '\\n---\\n'.join(sorted(set(x))),\n",
    "    # Concatenate all lists of original_index into a single list\n",
    "    'original_index': lambda x: sum(x, []),\n",
    "    # Collect all question_index values into a list\n",
    "    'question_index': lambda x: list(x)\n",
    "}).reset_index()\n",
    "\n",
    "# Create a new column 'answer_index' as a unique index for each answer\n",
    "df_answer_grouped['answer_index'] = df_answer_grouped.index\n",
    "\n",
    "df_answer_grouped.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "032e65de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save a mapping from question_index to question and original_index\n",
    "question_map = df_answer_grouped[['answer_index', 'answer', 'original_index']].set_index('answer_index').to_dict(orient='index')\n",
    "with open('../data/answer_index_map.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(question_map, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba3ff373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer length statistics:\n",
      "  Max length: 117627\n",
      "  Min length: 6\n",
      "  Mean length: 1461.92\n",
      "  Median length: 957.0\n",
      "  Standard deviation: 2200.87\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics about the length of the answers\n",
    "answer_lengths = df_answer_grouped['answer'].apply(len)\n",
    "print(\"Answer length statistics:\")\n",
    "print(f\"  Max length: {answer_lengths.max()}\")\n",
    "print(f\"  Min length: {answer_lengths.min()}\")\n",
    "print(f\"  Mean length: {answer_lengths.mean():.2f}\")\n",
    "print(f\"  Median length: {answer_lengths.median()}\")\n",
    "print(f\"  Standard deviation: {answer_lengths.std():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e60b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Aron\n",
      "[nltk_data]     Ifanger\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except Exception:\n",
    "    print(\"Downloading the 'punkt' sentence tokenizer from NLTK...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "except Exception:\n",
    "    print(\"Downloading the 'punkt_tab' sentence tokenizer from NLTK...\")\n",
    "    nltk.download('punkt_tab')\n",
    "\n",
    "\n",
    "\n",
    "def chunk_text(text: str, tokenizer, chunk_size: int = 400, chunk_overlap: int = 50) -> list[str]:\n",
    "    \"\"\"\n",
    "    Divide a long text into smaller chunks, respecting the sentence boundaries\n",
    "    and with a defined overlap in number of tokens.\n",
    "\n",
    "    Args:\n",
    "        text (str): The complete text to be divided.\n",
    "        tokenizer: The tokenizer from the Transformers library to be used to count the tokens.\n",
    "        chunk_size (int): The maximum size of each chunk in tokens.\n",
    "        chunk_overlap (int): The number of tokens of overlap between consecutive chunks.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of strings, where each string is a chunk of text.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # 1. Dividir o texto em sentenças usando NLTK\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # 2. Tokenizar todas as sentenças e guardar seus tokens\n",
    "    tokens = [tokenizer.encode(sentence, add_special_tokens=False) for sentence in sentences]\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk_tokens = []\n",
    "    current_chunk_sentences = []\n",
    "    \n",
    "    for i, sentence_tokens in enumerate(tokens):\n",
    "        # Se adicionar a próxima sentença ultrapassar o tamanho do chunk\n",
    "        if len(current_chunk_tokens) + len(sentence_tokens) > chunk_size:\n",
    "            # Finaliza o chunk atual e adiciona à lista\n",
    "            if current_chunk_tokens:\n",
    "                chunk_str = tokenizer.decode(current_chunk_tokens)\n",
    "                chunks.append(chunk_str.strip())\n",
    "            \n",
    "            # 3. Começa um novo chunk com sobreposição\n",
    "            # Pega os últimos tokens do chunk que acabamos de criar para formar a sobreposição\n",
    "            overlap_tokens = current_chunk_tokens[-chunk_overlap:] if chunk_overlap > 0 and current_chunk_tokens else []\n",
    "            current_chunk_tokens = overlap_tokens + sentence_tokens\n",
    "        else:\n",
    "            current_chunk_tokens.extend(sentence_tokens)\n",
    "\n",
    "    # Adiciona o último chunk que sobrou\n",
    "    if current_chunk_tokens:\n",
    "        chunk_str = tokenizer.decode(current_chunk_tokens)\n",
    "        chunks.append(chunk_str.strip())\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4af8bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (574 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>question</th>\n",
       "      <th>original_index</th>\n",
       "      <th>question_index</th>\n",
       "      <th>answer_index</th>\n",
       "      <th>answer_chunks</th>\n",
       "      <th>chunk_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A kidney stone is a solid piece of material ...</td>\n",
       "      <td>What to do for Kidney Stones in Adults ?</td>\n",
       "      <td>[15613]</td>\n",
       "      <td>[14255]</td>\n",
       "      <td>0</td>\n",
       "      <td>- a kidney stone is a solid piece of material ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A kidney stone is a solid piece of material ...</td>\n",
       "      <td>What to do for Kidney Stones in Adults ?</td>\n",
       "      <td>[15613]</td>\n",
       "      <td>[14255]</td>\n",
       "      <td>0</td>\n",
       "      <td>scientists do not believe that eating any spec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A kidney stone is a solid piece of material ...</td>\n",
       "      <td>What to do for Kidney Stones in Adults ?</td>\n",
       "      <td>[15613]</td>\n",
       "      <td>[14255]</td>\n",
       "      <td>0</td>\n",
       "      <td>##gnose kidney stones, the health care provide...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>- A kidney stone is a solid piece of material ...</td>\n",
       "      <td>What to do for Kidney Stones in Adults ?</td>\n",
       "      <td>[15613]</td>\n",
       "      <td>[14255]</td>\n",
       "      <td>0</td>\n",
       "      <td>, as well as whether they are causing pain or ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>- A person may prevent or delay some health pr...</td>\n",
       "      <td>What to do for Nutrition for Advanced Chronic ...</td>\n",
       "      <td>[15567]</td>\n",
       "      <td>[14268]</td>\n",
       "      <td>1</td>\n",
       "      <td>- a person may prevent or delay some health pr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              answer  \\\n",
       "0  - A kidney stone is a solid piece of material ...   \n",
       "0  - A kidney stone is a solid piece of material ...   \n",
       "0  - A kidney stone is a solid piece of material ...   \n",
       "0  - A kidney stone is a solid piece of material ...   \n",
       "1  - A person may prevent or delay some health pr...   \n",
       "\n",
       "                                            question original_index  \\\n",
       "0           What to do for Kidney Stones in Adults ?        [15613]   \n",
       "0           What to do for Kidney Stones in Adults ?        [15613]   \n",
       "0           What to do for Kidney Stones in Adults ?        [15613]   \n",
       "0           What to do for Kidney Stones in Adults ?        [15613]   \n",
       "1  What to do for Nutrition for Advanced Chronic ...        [15567]   \n",
       "\n",
       "  question_index  answer_index  \\\n",
       "0        [14255]             0   \n",
       "0        [14255]             0   \n",
       "0        [14255]             0   \n",
       "0        [14255]             0   \n",
       "1        [14268]             1   \n",
       "\n",
       "                                       answer_chunks  chunk_index  \n",
       "0  - a kidney stone is a solid piece of material ...            0  \n",
       "0  scientists do not believe that eating any spec...            0  \n",
       "0  ##gnose kidney stones, the health care provide...            0  \n",
       "0  , as well as whether they are causing pain or ...            0  \n",
       "1  - a person may prevent or delay some health pr...            1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the same tokenizer that you will use in your BERT/DistilBERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "df_answer_grouped['answer_chunks'] = df_answer_grouped['answer'].apply(\n",
    "    lambda x: chunk_text(x, tokenizer, chunk_size=100, chunk_overlap=20)\n",
    ")\n",
    "\n",
    "df_chunks = df_answer_grouped.explode('answer_chunks')\n",
    "df_chunks[\"chunk_index\"] = df_chunks.index\n",
    "\n",
    "df_chunks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb74fb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer length statistics:\n",
      "  Max length: 5926\n",
      "  Min length: 4\n",
      "  Mean length: 398.75\n",
      "  Median length: 392.0\n",
      "  Standard deviation: 159.81\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics about the length of the answers\n",
    "chunks_lengths = df_chunks['answer_chunks'].apply(len)\n",
    "print(\"Answer length statistics:\")\n",
    "print(f\"  Max length: {chunks_lengths.max()}\")\n",
    "print(f\"  Min length: {chunks_lengths.min()}\")\n",
    "print(f\"  Mean length: {chunks_lengths.mean():.2f}\")\n",
    "print(f\"  Median length: {chunks_lengths.median()}\")\n",
    "print(f\"  Standard deviation: {chunks_lengths.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e713a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the df_chunks DataFrame to a CSV file for later use\n",
    "df_chunks.to_csv(\"../data/intern_screening_dataset_chunks.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36bd88f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def chunk_text_by_sentence(text: str, tokenizer, chunk_size: int = 400, overlap_sentences: int = 1) -> list[str]:\n",
    "    \"\"\"\n",
    "    Splits a text into chunks, with overlap based on whole sentences.\n",
    "    This is a more robust approach to ensure text integrity.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "\n",
    "    # 1. Split the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    \n",
    "    # 2. Group sentences into chunks\n",
    "    chunks = []\n",
    "    current_chunk_sentences = []\n",
    "    current_chunk_tokens = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_tokens = tokenizer.tokenize(sentence)\n",
    "        \n",
    "        # If adding the next sentence exceeds the limit\n",
    "        if current_chunk_tokens + len(sentence_tokens) > chunk_size and current_chunk_sentences:\n",
    "            # Finalize the current chunk\n",
    "            chunks.append(\" \".join(current_chunk_sentences))\n",
    "            \n",
    "            # Start a new chunk with sentence overlap\n",
    "            current_chunk_sentences = current_chunk_sentences[-overlap_sentences:]\n",
    "            current_chunk_tokens = len(tokenizer.tokenize(\" \".join(current_chunk_sentences)))\n",
    "        \n",
    "        current_chunk_sentences.append(sentence)\n",
    "        current_chunk_tokens += len(sentence_tokens)\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_chunk_sentences:\n",
    "        chunks.append(\" \".join(current_chunk_sentences))\n",
    "        \n",
    "    return chunks\n",
    "\n",
    "# --- PREPARATION SCRIPT ---\n",
    "\n",
    "def create_squad_dataset_pipeline(df_original: pd.DataFrame, tokenizer, max_answer_len_tokens: int = 512):\n",
    "    \"\"\"\n",
    "    Complete pipeline to create the knowledge base (chunks) and SQuAD training data.\n",
    "    \"\"\"\n",
    "    # 1. Create the knowledge base (KB) with the new chunking function\n",
    "    print(\"Step 1: Aggregating answers to create long contexts...\")\n",
    "    df_contexts = df_original.groupby('question')['answer'].apply(lambda x: ' '.join(str(i) for i in x)).reset_index()\n",
    "    df_contexts.columns = ['topic', 'long_context']\n",
    "\n",
    "    print(\"Step 2: Applying sentence-based chunking to long contexts...\")\n",
    "    all_chunks = []\n",
    "    for context in tqdm(df_contexts['long_context']):\n",
    "        chunks = chunk_text_by_sentence(context, tokenizer, chunk_size=400, overlap_sentences=1)\n",
    "        all_chunks.extend(chunks)\n",
    "\n",
    "    unique_chunks = list(set(all_chunks))\n",
    "    print(f\"Knowledge base created with {len(unique_chunks)} unique chunks.\")\n",
    "\n",
    "    # 2. Create the SQuAD training dataset\n",
    "    training_data = []\n",
    "    print(f\"\\nStep 3: Generating training data from {len(df_original)} original rows...\")\n",
    "    \n",
    "    # Counter for debugging\n",
    "    found_in_chunk_count = 0\n",
    "\n",
    "    for _, row in tqdm(df_original.iterrows(), total=df_original.shape[0]):\n",
    "        question = str(row['question'])\n",
    "        answer_text = str(row['answer'])\n",
    "        answer_tokens_len = len(tokenizer.tokenize(answer_text))\n",
    "\n",
    "        if 0 < answer_tokens_len < max_answer_len_tokens:\n",
    "            # --- SCENARIO A: Short Answer ---\n",
    "            for chunk_context in unique_chunks:\n",
    "                # The 'in' search is now much more likely to work\n",
    "                if answer_text in chunk_context:\n",
    "                    start_index = chunk_context.find(answer_text)\n",
    "                    training_data.append({\n",
    "                        'context': chunk_context, 'question': question,\n",
    "                        'answers': {'text': [answer_text], 'answer_start': [start_index]}\n",
    "                    })\n",
    "                    found_in_chunk_count += 1\n",
    "                    break\n",
    "        elif answer_tokens_len >= max_answer_len_tokens:\n",
    "            # --- SCENARIO B: Long Answer ---\n",
    "            long_answer_as_context = answer_text\n",
    "            answer_spans = chunk_text_by_sentence(long_answer_as_context, tokenizer, chunk_size=150, overlap_sentences=1)\n",
    "            for span in answer_spans:\n",
    "                start_index = long_answer_as_context.find(span)\n",
    "                if start_index != -1:\n",
    "                    training_data.append({\n",
    "                        'context': long_answer_as_context, 'question': question,\n",
    "                        'answers': {'text': [span], 'answer_start': [start_index]}\n",
    "                    })\n",
    "\n",
    "    print(f\"\\nProcessing completed. Generated {len(training_data)} training examples.\")\n",
    "    print(f\"For Scenario A (short answers), {found_in_chunk_count} answers were found in a chunk.\")\n",
    "    return unique_chunks, training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "91becd9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Aggregating answers to create long contexts...\n",
      "Step 2: Applying sentence-based chunking to long contexts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 132/992 [00:00<00:00, 1312.54it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (544 > 512). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 992/992 [00:00<00:00, 1276.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base created with 1201 unique chunks.\n",
      "\n",
      "Step 3: Generating training data from 1000 original rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 954.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing completed. Generated 904 training examples.\n",
      "For Scenario A (short answers), 529 answers were found in a chunk.\n",
      "\n",
      "--- Example of Generated Training Data ---\n",
      "{\n",
      "  \"context\": \"Chilaiditi syndrome is a medical condition in which a portion of the colon is abnormally positioned between the liver and the diaphragm. Symptoms vary, but may include abdominal pain, nausea, vomiting, and small bowel obstruction. In many cases, there are no symptoms and the interposition is an incidental finding. When no symptoms are present, the clinical finding is called Chilaiditi's sign.. The underlying cause of Chilaiditi syndrome is unknown. Treatment is symptomatic and supportive.\",\n",
      "  \"question\": \"What is (are) Chilaiditi syndrome ?\",\n",
      "  \"answers\": {\n",
      "    \"text\": [\n",
      "      \"Chilaiditi syndrome is a medical condition in which a portion of the colon is abnormally positioned between the liver and the diaphragm. Symptoms vary, but may include abdominal pain, nausea, vomiting, and small bowel obstruction. In many cases, there are no symptoms and the interposition is an incidental finding. When no symptoms are present, the clinical finding is called Chilaiditi's sign.. The underlying cause of Chilaiditi syndrome is unknown. Treatment is symptomatic and supportive.\"\n",
      "    ],\n",
      "    \"answer_start\": [\n",
      "      0\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\n",
      "Training dataset saved to '../data/squad_training_data_fixed.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET_PATH = '../data/intern_screening_dataset.csv'  # SET THE PATH\n",
    "MODEL_CHECKPOINT = 'distilbert-base-uncased'\n",
    "SQUAD_DATA_PATH = '../data/squad_training_data_fixed.json'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "df_original = pd.read_csv(DATASET_PATH).dropna().sample(n=1000, random_state=42)  # Using a sample for faster execution\n",
    "\n",
    "unique_chunks, squad_data = create_squad_dataset_pipeline(df_original, tokenizer)\n",
    "\n",
    "print(\"\\n--- Example of Generated Training Data ---\")\n",
    "if squad_data:\n",
    "    print(json.dumps(squad_data[0], indent=2, ensure_ascii=False))\n",
    "    \n",
    "    with open(SQUAD_DATA_PATH, 'w', encoding='utf-8') as f:\n",
    "        json.dump(squad_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f\"\\nTraining dataset saved to '{SQUAD_DATA_PATH}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
